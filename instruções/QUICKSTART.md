# üöÄ In√≠cio R√°pido - Sistema LLaMA Chat

## ‚úÖ Status do Projeto

**Modelo Instalado**: TinyLlama 1.1B Chat (~2.1 GB)
**Status**: ‚úÖ Pronto para uso
**Testado**: ‚úÖ Funcionando corretamente

---

## üéØ Op√ß√µes de Uso

### Op√ß√£o 1: Testar o Modelo (Recomendado Primeiro)

```bash
python test_model.py
```

Isso ir√°:
- ‚úÖ Validar que o modelo est√° instalado
- ‚úÖ Testar 3 gera√ß√µes de texto
- ‚úÖ Verificar depend√™ncias

**Tempo estimado**: 30-60 segundos

---

### Op√ß√£o 2: Executar a Aplica√ß√£o Web

```bash
python main.py
```

Acesse: http://localhost:8000

**Funcionalidades**:
- üí¨ Chat interativo
- üåê Interface web moderna
- üìä Monitoramento do modelo

---

### Op√ß√£o 3: Usar Programaticamente

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Carregar modelo
model_path = "models/tinyllama-1.1b-chat"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# Gerar resposta
prompt = "What is machine learning?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

---

### Op√ß√£o 4: Docker

```bash
docker-compose up --build
```

Acesse: http://localhost:8000

---

## üì¶ Baixar Outro Modelo (Opcional)

```bash
python download_model.py
```

**Modelos dispon√≠veis**:
- TinyLlama 1.1B ‚úÖ (j√° instalado)
- Open LLaMA 3B (maior, melhor qualidade)
- Open LLaMA 7B (ainda maior)
- Meta LLaMA 2/3 (requer login no HuggingFace)

---

## üîß Comandos √öteis

### Verificar Status
```bash
# Ver arquivos do projeto
ls -lah

# Ver modelos instalados
ls -lah models/

# Ver tamanho do modelo
du -sh models/tinyllama-1.1b-chat/
```

### Instalar Depend√™ncias (se necess√°rio)
```bash
pip install -r requirements.txt
```

### Atualizar Depend√™ncias
```bash
pip install --upgrade transformers torch
```

---

## üêõ Problemas Comuns

### "Module not found: transformers"
```bash
pip install -r requirements.txt
```

### "CUDA out of memory"
```python
# Use CPU ao inv√©s de GPU
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="cpu"
)
```

### Modelo muito lento
1. Use GPU se dispon√≠vel
2. Reduza `max_length` nas gera√ß√µes
3. Considere modelo menor (TinyLlama j√° √© o menor)

---

## üìö Documenta√ß√£o Completa

- **[README.md](README.md)** - Documenta√ß√£o principal
- **[models/README.md](models/README.md)** - Guia de uso dos modelos
- **[REFACTORING_SUMMARY.md](REFACTORING_SUMMARY.md)** - Resumo da refatora√ß√£o
- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Solu√ß√£o de problemas

---

## üéì Exemplos de Uso

### Chat Simples
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = "models/tinyllama-1.1b-chat"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

while True:
    user_input = input("You: ")
    if user_input.lower() in ['exit', 'quit', 'bye']:
        break

    inputs = tokenizer(user_input, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"AI: {response}\n")
```

### Com LangChain
```python
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_path = "models/tinyllama-1.1b-chat"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_length=200)
llm = HuggingFacePipeline(pipeline=pipe)

# Usar
response = llm.invoke("Explain AI in simple terms")
print(response)
```

### API REST
```bash
# Iniciar servidor
python main.py

# Em outro terminal, testar:
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!"}'
```

---

## ‚ú® Pr√≥ximos Passos

1. ‚úÖ **Testar**: `python test_model.py`
2. ‚úÖ **Executar**: `python main.py`
3. ‚úÖ **Explorar**: Abra http://localhost:8000
4. ‚úÖ **Personalizar**: Edite `config.py` conforme necess√°rio
5. ‚úÖ **Documentar**: Leia [README.md](README.md) para recursos avan√ßados

---

## üí° Dicas

- **Performance**: Use GPU se dispon√≠vel (10-50x mais r√°pido)
- **Qualidade**: Modelos maiores = respostas melhores (mas mais lentos)
- **Mem√≥ria**: TinyLlama usa ~2.5GB RAM, modelos maiores usam muito mais
- **Tokens**: Mais tokens = respostas mais longas (mas mais lentas)

---

**Pronto para come√ßar!** üéâ

Execute `python test_model.py` para validar tudo!
