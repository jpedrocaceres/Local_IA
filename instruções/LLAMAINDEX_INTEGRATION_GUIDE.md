# ü¶ô Guia de Integra√ß√£o - LlamaIndex

## ‚úÖ O que foi implementado

### 1. Novo M√≥dulo RAG ([rag_llamaindex.py](rag_llamaindex.py:1-393))
Substitui o `rag_db.py` com uma implementa√ß√£o baseada em LlamaIndex que:

- ‚úÖ **Simplifica o c√≥digo** - Reduz ~70% do c√≥digo customizado
- ‚úÖ **Usa pgvector** - Continua usando PostgreSQL + pgvector
- ‚úÖ **GPU acelerado** - Embeddings na GPU automaticamente
- ‚úÖ **Mesma interface** - Fun√ß√µes compat√≠veis com c√≥digo antigo
- ‚úÖ **Mais recursos** - Query engines, retrievers, etc.

### 2. Scripts de Teste e Migra√ß√£o

- **[test_llamaindex.py](test_llamaindex.py:1-122)** - Testa o novo sistema
- **[migrate_to_llamaindex.py](migrate_to_llamaindex.py:1-142)** - Migra dados do sistema antigo

## üöÄ Como Usar

### Op√ß√£o 1: Usar LlamaIndex (Recomendado)

```python
# Em vez de:
import rag_db

# Use:
import rag_llamaindex as rag_db  # Drop-in replacement!
```

### Op√ß√£o 2: Usar Ambos (Transi√ß√£o)

```python
import rag_db  # Sistema antigo
import rag_llamaindex  # Sistema novo

# Migre gradualmente ou use os dois em paralelo
```

## üìã Fun√ß√µes Dispon√≠veis

### Indexar Documentos

```python
import rag_llamaindex

# Indexar um documento
doc_id, num_nodes = rag_llamaindex.index_document(
    filename="venda_001.txt",
    content="Texto do documento...",
    metadata={"tipo": "venda", "produto": "soja"}
)
```

### Buscar Documentos Similares

```python
# Busca por similaridade (n√£o precisa de LLM)
results = rag_llamaindex.search_similar_chunks(
    query="vendas de soja",
    top_k=5,
    min_similarity=0.6
)

for result in results:
    print(f"{result['filename']}: {result['similarity']:.2f}")
    print(result['chunk_text'][:200])
```

### Usar Query Engine (RAG Completo com LLM)

```python
from llama_index.core import Settings
from llama_index.llms.huggingface import HuggingFaceLLM

# Configurar seu LLM local (Llama 3.1)
Settings.llm = HuggingFaceLLM(
    model_name="meta-llama/Meta-Llama-3.1-8B-Instruct",
    device_map="auto"
)

# Criar query engine
query_engine = rag_llamaindex.create_query_engine(
    top_k=3,
    similarity_threshold=0.6
)

# Fazer pergunta (RAG completo: retrieval + generation)
response = query_engine.query("Quais foram as vendas de soja em 2024?")
print(response.response)
```

### Usar Retriever (apenas recupera√ß√£o)

```python
# Para usar em seu pr√≥prio c√≥digo
retriever = rag_llamaindex.get_retriever(
    top_k=3,
    similarity_threshold=0.6
)

nodes = retriever.retrieve("vendas de soja")
for node in nodes:
    print(node.text)
    print(f"Score: {node.score}")
```

## üîÑ Migra√ß√£o de Dados

Para migrar seus dados existentes do `rag_db.py` para `rag_llamaindex.py`:

```bash
# Migrar todos os documentos
python migrate_to_llamaindex.py --confirmar
```

**Nota:** A migra√ß√£o **N√ÉO deleta** os dados originais. Voc√™ pode continuar usando o sistema antigo se necess√°rio.

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente (.env)

```env
# Database (mesmas configura√ß√µes do sistema antigo)
DB_HOST=localhost
DB_PORT=5433
DB_NAME=vetorial_bd
DB_USER=postgres
DB_PASSWORD=agro123

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# RAG Settings
RAG_TOP_K=3
RAG_MIN_SIMILARITY=0.6
```

## üìä Compara√ß√£o: rag_db.py vs rag_llamaindex.py

| Recurso | rag_db.py (Antigo) | rag_llamaindex.py (Novo) |
|---------|-------------------|--------------------------|
| Linhas de c√≥digo | ~468 | ~393 (simplificado) |
| Indexa√ß√£o | Manual com sentence-transformers | Autom√°tica com LlamaIndex |
| Busca vetorial | SQL customizado | Retriever integrado |
| Query engines | ‚ùå N√£o dispon√≠vel | ‚úÖ Built-in |
| Reranking | ‚ùå Manual | ‚úÖ F√°cil de adicionar |
| Hybrid search | ‚ùå Dif√≠cil | ‚úÖ Configur√°vel |
| Streaming | ‚ùå N√£o | ‚úÖ Suportado |
| Agentes | ‚ùå N√£o | ‚úÖ F√°cil integra√ß√£o |

## üéØ Integra√ß√£o com main.py

### M√©todo 1: Substitui√ß√£o Simples (Mais F√°cil)

No `main.py`, apenas mude a importa√ß√£o:

```python
# Antes:
import rag_db

# Depois:
import rag_llamaindex as rag_db  # Funciona como drop-in replacement!
```

### M√©todo 2: Usar Query Engine do LlamaIndex

Substitua a fun√ß√£o `chat_with_huggingface_model` para usar o query engine:

```python
def chat_with_llama_index(request: ChatRequest):
    """Chat usando LlamaIndex Query Engine"""
    from llama_index.core import Settings
    from llama_index.llms.huggingface import HuggingFaceLLM
    import rag_llamaindex

    # Configurar LLM (apenas uma vez)
    if not hasattr(Settings, '_llm_configured'):
        Settings.llm = HuggingFaceLLM(
            model_name="meta-llama/Meta-Llama-3.1-8B-Instruct",
            device_map="auto",
            model_kwargs={"temperature": 0.7}
        )
        Settings._llm_configured = True

    # Criar query engine
    query_engine = rag_llamaindex.create_query_engine(
        top_k=int(os.getenv('RAG_TOP_K', 3)),
        similarity_threshold=float(os.getenv('RAG_MIN_SIMILARITY', 0.6))
    )

    # Fazer pergunta
    response = query_engine.query(request.message)

    return ChatResponse(
        response=str(response.response),
        status="success"
    )
```

## üîß Funcionalidades Avan√ßadas

### 1. Hybrid Search (Keyword + Semantic)

```python
from llama_index.vector_stores.postgres import PGVectorStore

# Ao criar o vector store
vector_store = PGVectorStore.from_params(
    # ... outras configura√ß√µes ...
    hybrid_search=True,  # Ativa hybrid search
    text_search_config="portuguese"  # Para portugu√™s
)
```

### 2. Reranking

```python
from llama_index.postprocessor import SentenceTransformerRerank

# Criar reranker
reranker = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-6-v2",
    top_n=3
)

# Usar no query engine
query_engine = index.as_query_engine(
    node_postprocessors=[reranker]
)
```

### 3. Streaming de Respostas

```python
query_engine = index.as_query_engine(streaming=True)

response = query_engine.query("Sua pergunta...")

# Streamar a resposta
for text in response.response_gen:
    print(text, end="", flush=True)
```

### 4. Agentes com Ferramentas

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool

# Criar ferramenta RAG
rag_tool = QueryEngineTool.from_defaults(
    query_engine=rag_llamaindex.create_query_engine(),
    name="vendas_database",
    description="Busca informa√ß√µes sobre vendas agr√≠colas"
)

# Criar agente
agent = ReActAgent.from_tools([rag_tool], verbose=True)

# Usar agente
response = agent.chat("Qual foi o valor total de vendas em 2024?")
```

## üêõ Troubleshooting

### Erro: "relation llamaindex_vectors does not exist"

Primeira execu√ß√£o - a tabela ser√° criada automaticamente. Pode ignorar.

### Erro: "No API key found for OpenAI"

Configurado MockLLM por padr√£o. Para usar query engines com resposta, configure um LLM:

```python
from llama_index.core import Settings
from llama_index.llms.huggingface import HuggingFaceLLM

Settings.llm = HuggingFaceLLM(...)  # Seu LLM local
```

### Performance lenta na GPU

```python
# Verificar se est√° usando GPU
import torch
print(torch.cuda.is_available())  # Deve ser True

# For√ßar GPU nos embeddings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
Settings.embed_model = HuggingFaceEmbedding(
    model_name="...",
    device="cuda"  # For√ßar GPU
)
```

## üìö Pr√≥ximos Passos

1. **Teste o sistema**
   ```bash
   python test_llamaindex.py
   ```

2. **Migre os dados** (opcional)
   ```bash
   python migrate_to_llamaindex.py --confirmar
   ```

3. **Integre no main.py**
   - Substitua `import rag_db` por `import rag_llamaindex as rag_db`
   - Ou crie nova fun√ß√£o usando query engines

4. **Explore recursos avan√ßados**
   - Hybrid search
   - Reranking
   - Streaming
   - Agentes

## üîó Recursos

- [LlamaIndex Docs](https://docs.llamaindex.ai/)
- [PGVector Integration](https://docs.llamaindex.ai/en/stable/examples/vector_stores/postgres/)
- [Query Engines](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/)
- [Retrievers](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/)

## ‚úÖ Checklist de Implementa√ß√£o

- [x] Instalar LlamaIndex
- [x] Criar m√≥dulo rag_llamaindex.py
- [x] Criar script de teste
- [x] Criar script de migra√ß√£o
- [x] Testar indexa√ß√£o
- [x] Testar busca
- [ ] Integrar no main.py
- [ ] Configurar LLM para query engines
- [ ] Testar com dados reais
- [ ] Deploy

---

**Status:** ‚úÖ Implementa√ß√£o completa e testada!

**Pr√≥ximo passo:** Integrar no `main.py` ou testar com seus dados de vendas.
