# Sistema de Chat Multi-Modelo - GPU Only

Sistema de chat com m√∫ltiplos modelos de IA: Llama 3.1 8B e TRM (Tiny Recursive Models), usando PyTorch com processamento exclusivo em GPU e interface web moderna.

## üéØ Modelos Dispon√≠veis

### Llama 3.1 8B Instruct
‚úÖ **Par√¢metros**: 8 bilh√µes
‚úÖ **Tamanho**: ~16 GB (FP16)
‚úÖ **Uso**: Chat geral, instru√ß√µes, conversa√ß√£o
‚úÖ **VRAM**: 8-16GB

### TRM (Tiny Recursive Models)
‚úÖ **Par√¢metros**: 7 milh√µes
‚úÖ **Tamanho**: ~28 MB
‚úÖ **Uso**: Racioc√≠nio, puzzles, tarefas l√≥gicas
‚úÖ **VRAM**: 2-4GB

## üîÑ Sele√ß√£o de Modelo

Voc√™ pode escolher qual modelo usar antes de iniciar o servidor:

```bash
python select_model.py
```

## üöÄ Funcionalidades

- ü¶ô **Llama 3.1 8B**: Modelo de linguagem avan√ßado da Meta
- üß† **TRM**: Modelo recursivo compacto para racioc√≠nio
- üîÑ **Sele√ß√£o de Modelo**: Escolha o modelo antes de executar
- üí¨ **Chat Interativo**: Interface web moderna e responsiva
- ‚ö° **GPU Only**: Processamento exclusivo em GPU NVIDIA
- üîß **M√∫ltiplos Backends**: Suporta Hugging Face, GGUF, Ollama e TRM
- üì¶ **Extens√≠vel**: Arquitetura modular e configur√°vel

## üìã Pr√©-requisitos

### Obrigat√≥rios
- Python 3.10+
- **NVIDIA GPU com CUDA 11.8+** (obrigat√≥rio)
- 16GB+ VRAM (GPU)
- 32GB+ RAM (sistema)
- 20GB+ espa√ßo em disco

### Software
- CUDA Toolkit 11.8+
- cuDNN 8.x
- Docker (opcional)

## üèÅ In√≠cio R√°pido

### 1. Verificar GPU

```bash
# Verificar se CUDA est√° dispon√≠vel
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

### 2. Instalar Depend√™ncias

```bash
# Criar ambiente virtual
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Instalar depend√™ncias
pip install -r requirements.txt
```

### 3. Selecionar Modelo

```bash
python select_model.py
```

Escolha entre:
- **[1] Llama 3.1 8B** - Para chat geral e instru√ß√µes
- **[2] TRM** - Para tarefas de racioc√≠nio e puzzles

### 4. Configurar o Modelo Escolhido

#### Se escolheu Llama 3.1 (escolha uma op√ß√£o)

**Op√ß√£o A: Ollama (Recomendado)**
```bash
# Instalar Ollama: https://ollama.ai
ollama pull llama3.1:8b
```

**Op√ß√£o B: Hugging Face**
```bash
# 1. Login no Hugging Face
huggingface-cli login

# 2. Aceitar termos em: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct

# 3. Baixar modelo
python download_model.py
```

#### Se escolheu TRM

```bash
# 1. Executar setup do TRM
python setup_trm.py

# 2. Treinar modelo (ou baixar checkpoint pr√©-treinado)
cd models/TRM
python pretrain.py arch=trm data_paths="[data/arc1concept-aug-1000]"
```

Para mais detalhes sobre TRM, veja [TRM_GUIDE.md](TRM_GUIDE.md)

### 5. Executar a Aplica√ß√£o

```bash
python main.py
```

Acesse: http://localhost:8000

## üîß Configura√ß√£o

### Vari√°veis de Ambiente

Crie um arquivo `.env`:

```env
# GPU (obrigat√≥rio)
CUDA_VISIBLE_DEVICES=0
GPU_ONLY_MODE=true

# Modelo
MODEL_NAME=meta-llama/Meta-Llama-3.1-8B-Instruct
MODEL_PATH=models/llama3.1-8b

# API
PORT=8000
HOST=0.0.0.0
```

### Backends Suportados

O sistema detecta automaticamente o backend dispon√≠vel:

1. **GGUF** - Se encontrar arquivo `.gguf` no diret√≥rio do modelo
2. **Ollama** - Se Ollama estiver rodando em `http://localhost:11434`
3. **Hugging Face** - Fallback padr√£o

## üì¶ Modelos

### Llama 3.1 8B Instruct

**Especifica√ß√µes:**
- Par√¢metros: 8 bilh√µes
- Tamanho: ~16 GB (FP16)
- Contexto: 128K tokens (ajust√°vel)
- Licen√ßa: Llama 3.1 Community License
- Uso: Chat, instru√ß√µes, reasoning

**Links:**
- Hugging Face: `meta-llama/Meta-Llama-3.1-8B-Instruct`
- Ollama: `llama3.1:8b`

## üíª Uso Program√°tico

### B√°sico com Transformers

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = "meta-llama/Meta-Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"  # GPU autom√°tico
)

# Gerar texto
prompt = "What is artificial intelligence?"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=256)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### Com Ollama

```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "llama3.1:8b",
        "prompt": "Explain quantum computing",
        "stream": False
    }
)
print(response.json()["response"])
```

### Com GGUF

```python
from llama_cpp import Llama

llm = Llama(
    model_path="models/llama3.1-8b/model.gguf",
    n_gpu_layers=-1,  # Usar todas as camadas na GPU
    n_ctx=8192
)

output = llm(
    "Hello!",
    max_tokens=256,
    temperature=0.7
)
print(output["choices"][0]["text"])
```

## üê≥ Docker

### Op√ß√£o 1: Docker Compose (Recomendado)

```bash
docker-compose up --build
```

### Op√ß√£o 2: Docker Manual

```bash
docker build -t llama-chat .
docker run --gpus all -p 8000:8000 -v ./models:/app/models llama-chat
```

**Nota**: Docker requer NVIDIA Container Toolkit para suporte a GPU.

## üìö API Endpoints

### Health Check
```bash
GET /api/health

Response:
{
  "status": "healthy",
  "mode": "GPU_ONLY",
  "model_loaded": true,
  "model_type": "ollama",
  "device": "cuda",
  "cuda_available": true,
  "gpu_info": {
    "device_name": "NVIDIA GeForce RTX 3090",
    "memory_total": "24.0 GB",
    "memory_allocated": "15.2 GB",
    "memory_cached": "16.1 GB"
  }
}
```

### Chat
```bash
POST /api/chat
Content-Type: application/json

{
  "message": "Explain quantum computing in simple terms"
}

Response:
{
  "response": "Quantum computing is...",
  "status": "success"
}
```

## üìÅ Estrutura do Projeto

```
ML/
‚îú‚îÄ‚îÄ models/                      # Modelos LLaMA
‚îÇ   ‚îú‚îÄ‚îÄ llama3.1-8b/            # Llama 3.1 8B (local)
‚îÇ   ‚îî‚îÄ‚îÄ README.md               # Guia de modelos
‚îú‚îÄ‚îÄ mcp/                         # MCP Server
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ static/                      # Interface web
‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ main.py                      # Aplica√ß√£o FastAPI
‚îú‚îÄ‚îÄ config.py                    # Configura√ß√µes GPU
‚îú‚îÄ‚îÄ setup.py                     # Setup inicial
‚îú‚îÄ‚îÄ download_model.py            # Download de modelos
‚îú‚îÄ‚îÄ test_model.py                # Testes
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias Python
‚îú‚îÄ‚îÄ Dockerfile                   # Docker config
‚îú‚îÄ‚îÄ docker-compose.yml           # Docker Compose
‚îú‚îÄ‚îÄ QUICKSTART.md                # Guia r√°pido
‚îú‚îÄ‚îÄ TROUBLESHOOTING.md           # Solu√ß√£o de problemas
‚îî‚îÄ‚îÄ README.md                    # Este arquivo
```

## ‚ö° Otimiza√ß√µes

### 1. Quantiza√ß√£o 4-bit (Reduz VRAM)

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 2. Flash Attention 2 (Mais R√°pido)

```bash
pip install flash-attn --no-build-isolation
```

```python
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="flash_attention_2"
)
```

### 3. Compila√ß√£o (PyTorch 2.0+)

```python
import torch

model = torch.compile(model)  # Acelera infer√™ncia
```

## üêõ Troubleshooting

### GPU n√£o detectada
```bash
# Verificar CUDA
python -c "import torch; print(torch.cuda.is_available())"
python -c "import torch; print(torch.cuda.get_device_name(0))"

# Reinstalar PyTorch com CUDA
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Out of Memory (OOM)
**Solu√ß√µes:**
1. Use quantiza√ß√£o 4-bit (ver Otimiza√ß√µes)
2. Reduza `max_new_tokens` em gera√ß√µes
3. Use modelo GGUF com menos camadas na GPU
4. Feche outros programas usando VRAM

### Ollama n√£o conecta
```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Iniciar Ollama
ollama serve

# Verificar modelo instalado
ollama list
```

### Modelo lento
**Solu√ß√µes:**
1. Use Ollama (otimizado)
2. Ative Flash Attention 2
3. Reduza contexto (`max_length`)
4. Use compila√ß√£o PyTorch

## üîÑ Trocar de Modelo

Para alternar entre Llama 3.1 e TRM:

1. **Parar o servidor** (Ctrl+C)
2. **Executar**: `python select_model.py`
3. **Selecionar novo modelo**
4. **Reiniciar**: `python main.py`

## üìñ Documenta√ß√£o Adicional

- [TRM_GUIDE.md](TRM_GUIDE.md) - Guia completo do TRM
- [QUICKSTART.md](QUICKSTART.md) - Guia de in√≠cio r√°pido
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - Solu√ß√£o de problemas
- [Llama 3.1 Model Card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)
- [TRM Repository](https://github.com/SamsungSAILMontreal/TinyRecursiveModels)
- [Transformers Docs](https://huggingface.co/docs/transformers)
- [Ollama Docs](https://ollama.ai/docs)

## üîí Seguran√ßa

- A aplica√ß√£o roda em `0.0.0.0:8000` por padr√£o
- Para produ√ß√£o, adicione autentica√ß√£o (JWT, OAuth)
- Use HTTPS em produ√ß√£o (nginx + certbot)
- Valide e sanitize inputs do usu√°rio
- Implemente rate limiting
- Monitore uso de GPU e custos

## üìù Licen√ßa

- Projeto: MIT License
- Llama 3.1: [Llama 3.1 Community License](https://ai.meta.com/llama/license/)

**Importante**: Revise e aceite os termos de uso do Llama 3.1 antes de usar em produ√ß√£o.

## üîå MCP Server (Model Context Protocol)

O projeto inclui um **servidor MCP** com ferramentas especializadas para agroneg√≥cio que podem ser usadas pelo Claude Desktop e outros clientes MCP.

### Ferramentas Dispon√≠veis

#### üå§Ô∏è Previs√£o do Tempo Agr√≠cola
Consulta previs√£o do tempo com recomenda√ß√µes espec√≠ficas para agricultura.

```python
# Exemplo via Claude Desktop
"Qual a previs√£o do tempo para Corumb√°, MS para os pr√≥ximos 7 dias?"
```

#### üí∞ Pre√ßos de Commodities
Consulta pre√ßos de soja, milho, caf√©, boi gordo e outras commodities com an√°lise de mercado.

```python
# Exemplo via Claude Desktop
"Qual o pre√ßo atual da soja e me d√™ recomenda√ß√µes de comercializa√ß√£o?"
```

#### üìÖ Calend√°rio Agr√≠cola
Fornece informa√ß√µes sobre √©pocas de plantio e colheita regionalizadas.

```python
# Exemplo via Claude Desktop
"Quando plantar soja no Centro-Oeste? Estamos em qual fase do calend√°rio?"
```

### Configura√ß√£o do MCP

1. **Instalar depend√™ncias:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configurar Claude Desktop:**

   Edite `%APPDATA%\Claude\claude_desktop_config.json` (Windows) ou `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS):

   ```json
   {
     "mcpServers": {
       "pantanal-agricola": {
         "command": "python",
         "args": ["C:\\caminho\\completo\\mcp_server.py"]
       }
     }
   }
   ```

3. **Reiniciar Claude Desktop**

üìñ **Guia Completo:** [MCP_SETUP.md](MCP_SETUP.md)

## üí° Roadmap

- [x] ‚úÖ Suporte a m√∫ltiplos modelos (Llama + TRM)
- [x] ‚úÖ Sistema de sele√ß√£o de modelo interativo
- [x] ‚úÖ Integra√ß√£o TRM para racioc√≠nio
- [ ] Suporte a streaming de respostas
- [ ] Sistema de mem√≥ria/contexto persistente
- [ ] Interface web para sele√ß√£o de modelo
- [ ] Integra√ß√£o com pgvector para RAG
- [ ] API de embeddings
- [ ] Multi-GPU support
- [ ] Quantiza√ß√£o autom√°tica baseada em VRAM
- [ ] Ensemble de modelos (Llama + TRM)

## üìû Suporte

Problemas comuns e solu√ß√µes:

1. Verifique logs: `python main.py` ou `docker-compose logs`
2. Teste GPU: `python -c "import torch; print(torch.cuda.is_available())"`
3. Verifique health: `curl http://localhost:8000/api/health`
4. Consulte [TROUBLESHOOTING.md](TROUBLESHOOTING.md)
5. Teste modelo: `python test_model.py`

---

**Made with ‚ù§Ô∏è using Llama 3.1 and Transformers**
